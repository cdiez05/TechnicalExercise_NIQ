{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "For the second part, we introduce you a task consisting in extracting information (entity tagging and classification) from images of products. You can focus on the nutritional facts as in the case of EDA. Your main goal is to develop pseudo code for the data processing and ML solution pipeline to automate the extraction. Please create a remote git repository with your pseudocode. It needs to illustrate the solution strategy and demonstrate good code practices. There is no need to implement fully working code or ML models. We will conduct a set of questions based on your solution and the repository created.\n",
    "\n",
    "Requested functionalities:\n",
    "\n",
    "o Ability to split data in multiple custom ways to train and evaluate the system\n",
    "\n",
    "o Ability to handle different Machine Learning tasks: classification of single-label data objects, classification multi-label data objects, entity tagging.\n",
    "\n",
    "o Ability to load, pre-process multimodal data (image and text) o Ability to handle the data in a PyTorch data loader\n",
    "\n",
    "o Ability to define the model training and inference stages for the tasks (you can approach it as single or multitask model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Tasks:\n",
    "\n",
    "\n",
    "1. Single-Label Classification: Classifying Product Category\n",
    "- Task: Given an image of a product, classify it into one of several categories (e.g., yogurt, cheese, biscuits).\n",
    "\n",
    "- Input: Images of products, loaded from df['image_url'].\n",
    "- Target Output: A single category label for each product, obtained from df['main_category_en'].\n",
    "- Dataset Requirements:\n",
    "    - The dataset should have non-empty values for df['image_url'] (the image of the product).\n",
    "    - It should also have non-empty values for df['main_category_en'] (the product category label).\n",
    "\n",
    "\n",
    "2. Multi-Label Classification: Predicting Nutritional Labels (High Calories, High Fat, High Protein)\n",
    "- Task: Given an image of a product's nutritional table, classify whether the product is high in calories, high in fat, and/or high in protein. This is a multi-label classification problem because a product can belong to more than one category.\n",
    "\n",
    "- Input: Images of nutritional tables, loaded from df['image_nutrition_url'].\n",
    "- Target Output: Three binary labels:\n",
    "    - High_calories: Whether the product is high in calories (binary: 0 or 1).\n",
    "    - High_fat: Whether the product is high in fat (binary: 0 or 1).\n",
    "    - High_protein: Whether the product is high in protein (binary: 0 or 1).\n",
    "- Dataset Requirements:\n",
    "    - Non-empty values for df['image_nutrition_url'] (the image of the nutritional table).\n",
    "    - Ensure the nutritional values are present (e.g., fat_100g, sugars_100g, proteins_100g), which are used to generate the binary labels.\n",
    "\n",
    "3. Entity Tagging: Labeling Ingredients in Product Ingredients Images\n",
    "- Task: Perform entity tagging (Named Entity Recognition - NER) on the ingredients list provided in the product ingredients description image. The first problem here is to extract the text and for that we decided to use an OCR reader (did not implement it since it was straightfroward with the use of some already implemented APIs). Once we have the text extracted, the goal is to identify and label different entities in the text such as ingredients, amounts, and other relevant tokens. [Note: we used the ingredients description in text directly to avoid the text extraction step, however, using an OCR reader would be straightforward]\n",
    "\n",
    "- Input: A textual description of the ingredients list, loaded from df['ingredients_text']. This is typically a string representing the ingredients of the product.\n",
    "\n",
    "- Target Output: Entity labels for each token (word) in the ingredients text. For example:\n",
    "\n",
    "    - B-ING: Beginning of an ingredient entity.\n",
    "    - I-ING: Continuation of an ingredient entity.\n",
    "    - B-AMT: Beginning of an amount entity (such as a percentage or quantity).\n",
    "    - O: Token that is outside of any entity (i.e., not relevant to ingredients or amounts).\n",
    "\n",
    "- Dataset Requirements:\n",
    "\n",
    "    - The dataset must have non-empty entries in df['ingredients_text'] to provide valid text for the entity tagging task.\n",
    "    - Rows with missing or NaN values in the ingredients_text column should be dropped to ensure only valid texts are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config file (.yaml)\n",
    "\n",
    "Following Javier's indication from the previous meeting I decided to create a .yaml file to set the configuration. This allows for a centralized and flexible management of various configuration settings such as hyperparameters tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load configuration from config file (.yaml)\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "# Load the config file\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "# Validation Image URL: we discard the urls that do not work\n",
    "def filter_valid_urls(df, column, timeout=15):\n",
    "    valid_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        image_url = row[column]\n",
    "        print(f\"Checking URL: {image_url}\")\n",
    "        try:\n",
    "            response = requests.get(image_url, timeout=timeout)\n",
    "            response.raise_for_status()  \n",
    "            valid_rows.append(row)  # Able to access the url\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Skipping URL due to error: {image_url}, Error: {e}\") #We skip the urls that have access problems\n",
    "    \n",
    "    # Create a new dataframe with only valid urls\n",
    "    return pd.DataFrame(valid_rows)\n",
    "\n",
    "# Data Preprocessing\n",
    "def load_and_preprocess_image(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "\n",
    "        # Convert image to RGB if it is not already in RGB format (format that Resnet has to process images)\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        preprocess_transform = transforms.Compose([\n",
    "            transforms.Resize(tuple(config['data']['image_size'])),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        return preprocess_transform(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load image from URL: {image_url}, Error: {e}\")\n",
    "        return None  # Handle invalid images\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Custom Dataset class for task 2: here we select the features we want to include to each task (single-label and multi-label classification)\n",
    "class Task2CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, task_type='single_label_classification', transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.task_type = task_type\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Assign an index to each category: dictionary\n",
    "        if task_type == 'single_label_classification':\n",
    "            self.single_label_mapping = {category: idx for idx, category in enumerate(dataframe['main_category_en'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        if self.task_type == 'single_label_classification':\n",
    "            image = load_and_preprocess_image(row['image_url'])\n",
    "            if image is not None:\n",
    "                # Use the dict to map the category to its corresponding index\n",
    "                label = self.single_label_mapping[row['main_category_en']]\n",
    "                return image, label\n",
    "\n",
    "        elif self.task_type == 'multi_label_classification':\n",
    "            image = load_and_preprocess_image(row['image_nutrition_url'])\n",
    "            label = torch.tensor([row['High_Fat'], row['High_Sugar'], row['High_Protein']], dtype = torch.float)\n",
    "            return image, label\n",
    "\n",
    "\n",
    "# Data splitting\n",
    "def split_data(dataframe):\n",
    "    train_df, temp_df = train_test_split(dataframe, test_size=config['data']['split_ratios']['val'] + config['data']['split_ratios']['test'], random_state=config['data']['seed'])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=config['data']['split_ratios']['test'] / (config['data']['split_ratios']['val'] + config['data']['split_ratios']['test']), random_state=config['data']['seed'])\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Function to create the 3 dataloaders: train, val, test loaders\n",
    "def create_dataloaders(train_df, val_df, test_df, task_type):\n",
    "    train_dataset = Task2CustomDataset(train_df, task_type=task_type)\n",
    "    val_dataset = Task2CustomDataset(val_df, task_type=task_type)\n",
    "    test_dataset = Task2CustomDataset(test_df, task_type=task_type)\n",
    "    \n",
    "\n",
    "    ### Had to avoid using num_workers because it wasnt compatible with my laptop\n",
    "\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=config['data']['batch_size'], shuffle=True, num_workers=config['data']['num_workers'])\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=config['data']['batch_size'], shuffle=False, num_workers=config['data']['num_workers'])\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=config['data']['batch_size'], shuffle=False, num_workers=config['data']['num_workers'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['data']['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['data']['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['data']['batch_size'], shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Evaluation: accuracy calculation\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            if images is not None:\n",
    "                images, labels = images.to(device), labels.to(device)  # Move images and labels to device\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy}%')\n",
    "    return accuracy\n",
    "\n",
    "# Training\n",
    "def train_model(model, train_loader, val_loader, device, criterion, optimizer, epochs=10):\n",
    "    model.to(device)  # Move the model to the device\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            if images is not None and labels is not None:\n",
    "                images, labels = images.to(device), labels.to(device)  # Move images and labels to the device\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Validation set accuracy: {evaluate_model(model, val_loader)}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    compute_accuracy(model, test_loader, device= device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description:\n",
    "\n",
    "- Backbone: ResNet-18, a deep convolutional neural network (CNN) architecture that was designed to enable training very deep networks by introducing residual connections, is used as the backbone in both models due to its strong balance of performance and efficiency. Reasons why I chose Resnet18:\n",
    "\n",
    "    - Pretrained Model: ResNet-18 is widely available as a pretrained model on large-scale datasets like ImageNet, providing a strong feature extraction capability. Additionally, pretrained models often generalize well to new tasks.\n",
    "\n",
    "    - Residual Connections: The main innovation in ResNet is its use of skip connections, which help mitigate the vanishing gradient problem and allow the network to train effectively even with many layers. This enables deep learning models to have improved performance by making it easier to propagate gradients during backpropagation.\n",
    "\n",
    "    - Computational Efficiency: ResNet-18, compared to deeper variants like ResNet-50 or ResNet-152, strikes a good balance between computational cost and accuracy. Having into account that had to run everything locally (just making sure the code was working - running at least) I decided to use the ResNet18.\n",
    "\n",
    "\n",
    "- Use of Sigmoid for Multi-Label Classification: the sigmoid activation function is used at the final layer of the network to convert the raw logits into probabilities. In multi-label classification, each label (e.g., high-calories, high-fat, high-protein) is treated as a separate binary classification problem. Sigmoid outputs a probability for each label independently, allowing the model to predict multiple labels for a single input. The sigmoid function maps the logits to values between 0 and 1, which can be interpreted as the probability that each label is present.\n",
    "\n",
    "- We decided to use Linear layers as the heads for each task after backbone due to efficiency and simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SingleLabelClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SingleLabelClassificationModel, self).__init__()\n",
    "        # Load pretrained ResNet-18 as the backbone: using hf_cache in stored in current folder (easier to clean cache)\n",
    "        cache_path = './hf_cache/'\n",
    "        if not os.path.exists(cache_path):\n",
    "            os.makedirs(cache_path)\n",
    "\n",
    "        torch.hub.set_dir(cache_path)\n",
    "\n",
    "\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        \n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "\n",
    "class MultiLabelClassificationModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelClassificationModel, self).__init__()\n",
    "        # Load pretrained ResNet-18 as the backbone: using hf_cache in stored in current folder (easier to clean cache)\n",
    "        cache_path = './hf_cache/'\n",
    "        if not os.path.exists(cache_path):\n",
    "            os.makedirs(cache_path)\n",
    "\n",
    "        torch.hub.set_dir(cache_path)\n",
    "\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        \n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, num_labels)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.backbone(x)\n",
    "        return self.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-label classification\n",
    "\n",
    "1. Read the dataset and filter to ensure valid urls\n",
    "2. Split training, validation, test sets -> (PyTorch) Dataloeaders\n",
    "3. Model initialisation: resnet18 as backbone and Linear Layer as classification head\n",
    "4. We add torch device to enable the use of GPUs when possible\n",
    "4. Model training: CrossEntropyLoss is used for training using Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10000\n",
    "\n",
    "# Read dataframe\n",
    "df = pd.read_csv(config['data']['dataset_paths']['dataset_csv'], delimiter='\\t',on_bad_lines='skip', low_memory=False, nrows=nrows)\n",
    "\n",
    "column_single_label_image = 'image_url'\n",
    "df_classification = df[df[column_single_label_image].notna() & df['main_category_en'].notna()]\n",
    "\n",
    "\n",
    "# Split the dataset: 0.7 training, 0.15 val, 0.15 test\n",
    "train_df, val_df, test_df = split_data(df_classification)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(train_df, val_df, test_df, task_type='single_label_classification')\n",
    "\n",
    "criterion_single = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_classes = df_classification['main_category_en'].nunique()\n",
    "single_label_model = SingleLabelClassificationModel(num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(single_label_model.parameters(), lr=config['model']['learning_rate'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "train_model(single_label_model, train_loader, val_loader, criterion, optimizer, device = device, epochs = config['model']['epochs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification\n",
    "\n",
    "1. Read the dataset and filter to ensure valid urls\n",
    "2. Filtering: non-missing values for the nutritional columns (fat_100g, sugars_100g, proteins_100g). Nutritional values transformed into binary labels -> *Multi-label classification*\n",
    "2. Split training, validation, test sets -> (PyTorch) Dataloeaders\n",
    "3. Model initialisation: resnet18 as backbone and Linear Layer as classification head\n",
    "4. We add torch device to enable the use of GPUs when possible\n",
    "5. Model training: Binary Cross-Entropy with Logits Loss is used for training with Adam optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We dont read dataframe the dataframe again in order to use the same we were using for single-label classification\n",
    "\n",
    "nutrition_columns = ['fat_100g', 'sugars_100g', 'proteins_100g']\n",
    "column_multi_label_image = 'image_nutrition_url'\n",
    "\n",
    "\n",
    "# Data preprocessing: valid urls + Rows drop with missing nutritional values (fat, sugars, and proteins)\n",
    "df_multi_classification = df[df[column_multi_label_image].notna()]\n",
    "df_filtered = df_multi_classification.dropna(subset=nutrition_columns)\n",
    "\n",
    "\n",
    "# Nutritional values -> binary labels (thresholds should be discussed with the team or client)\n",
    "df_filtered['High_Fat'] = pd.to_numeric((df_filtered[nutrition_columns[0]] > 10).astype(int), errors='coerce')\n",
    "df_filtered['High_Sugar'] = pd.to_numeric((df_filtered[nutrition_columns[1]] > 5).astype(int), errors='coerce')\n",
    "df_filtered['High_Protein'] = pd.to_numeric((df_filtered[nutrition_columns[2]] > 7.5).astype(int), errors='coerce')\n",
    "num_labels = len(nutrition_columns)\n",
    "\n",
    "# Split the dataset: 0.7 training, 0.15 val, 0.15 test\n",
    "train_df, val_df, test_df = split_data(df_filtered)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(train_df, val_df, test_df, task_type='multi_label_classification')\n",
    "\n",
    "\n",
    "criterion_multi = torch.nn.BCEWithLogitsLoss()\n",
    "multi_label_model = MultiLabelClassificationModel(num_labels)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "train_model(single_label_model, train_loader, val_loader, criterion, optimizer, device = device, epochs = config['model']['epochs'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity tagging\n",
    "\n",
    "- Task: Perform entity tagging (Named Entity Recognition - NER) on the ingredients list provided in the product ingredients description image. The first problem here is to extract the text and for that we decided to use an OCR reader (did not implement it since it was straightfroward with the use of some already implemented APIs). Once we have the text extracted (we used 'ingredients_text' column where we had this exact info), the goal is to identify and label different entities in the text such as ingredients, amounts, and other relevant tokens. [Note: we used the ingredients description in text directly to avoid the text extraction step, however, using an OCR reader would be straightforward]\n",
    "\n",
    "\n",
    "We want to implement a supervised learning model, and for that we need to have ground truth data to compare model's output with the ground truth and then adjust the model's parameters accordingly. Since we do not have grounf truth data available it may require manual labeling. Manual labeling for tasks like entity tagging is time-consuming and labor-intensive. Typically, domain experts or annotators would need to go through each product’s ingredient list and manually tag ingredients and amounts. I decided to use the openai api to avoid wasting time doing the manuak labeling. We used the model GPT-4 which is asked to generate a labeled dataset quickly. The output is used to train machine learning model, in this case the BERT transformer.\n",
    "\n",
    "(Other way which is less computationally intensive and more efficient to solve this task is just using the output of the openai api. However, we would need to explore how the openai model performs and see if it is trustworthy)\n",
    "\n",
    "Setup\n",
    "1. Loading config file (.yaml) and using openai_api_key (Config class)\n",
    "2. Split training, validation, test sets -> (PyTorch) Dataloeaders\n",
    "3. Ground truth collection: using openai api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration and other auxilary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import openai\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load configuration using config.yaml and the class Config to use the openai_api_key\n",
    "def load_config(config_path='config.yaml'):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "# Configuration class to manage the loaded configuration\n",
    "class Config:\n",
    "    def __init__(self, config_path='config.yaml'):\n",
    "        self.config = load_config(config_path)\n",
    "        openai.api_key = 'openai_api_key'  # Add your OpenAI API key here\n",
    "    \n",
    "    def get(self, key):\n",
    "        return self.config.get(key)\n",
    "\n",
    "# Function to split the data according to config.yaml ratios\n",
    "def split_data(dataframe, config):\n",
    "    train_ratio = config['data']['split_ratios']['train']\n",
    "    val_ratio = config['data']['split_ratios']['val']\n",
    "    test_ratio = config['data']['split_ratios']['test']\n",
    "    \n",
    "    train_df, test_df = train_test_split(dataframe, test_size=(val_ratio + test_ratio), random_state=config['data']['seed'])\n",
    "    val_df, test_df = train_test_split(test_df, test_size=test_ratio / (test_ratio + val_ratio), random_state=config['data']['seed'])\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Function to label ingredients using OpenAI API\n",
    "def label_ingredients(text):\n",
    "    # OpenAI API call for entity tagging/classification\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-4\",\n",
    "        prompt=f\"Label the following text using BIO tagging: {text}. Use the following tags: 'B-ING' for beginning of an ingredient, 'I-ING' for continuation of an ingredient, 'B-AMT' for amount, 'O' for other tokens.\",\n",
    "        max_tokens=100\n",
    "    )\n",
    "    # Parse and return response\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# Function to preprocess and label dataset using OpenAI API\n",
    "def preprocess_and_label(dataframe):\n",
    "    dataframe['entity_tags'] = dataframe['ingredients_text'].apply(label_ingredients)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom new dataset and creating dataloader\n",
    "\n",
    "1. IngredientsDataset is a class to create the PyTorch Dataset needed for entity tagging task:\n",
    "- The input text (ingredients) is found in the ingredients_text column.\n",
    "- The ground truth BIO tags are in the entity_tags column, labeled by the OpenAI API. (Maybe other approaches can be used here like manual labeling)\n",
    "- The data is tokenized by splitting the input text into words (tokens) in the same way as the ground truth tags, with one token per word\n",
    "- It handles padding and truncation to ensure that each sequence is the same length (max_len)\n",
    "- Converts the tokens to input IDs (using a tokenizer) and creates an attention mask (where [PAD] tokens are ignored)\n",
    "\n",
    "2. DataLoader creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Custom Dataset -> Entity Tagging\n",
    "class IngredientsDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, label_to_id):\n",
    "        self.data = dataframe['ingredients_text'].dropna().reset_index(drop=True)\n",
    "        self.labels = dataframe['entity_tags'].dropna().reset_index(drop=True)  # Ground truth saved in entity_tags column (labeled with OpenAI)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_to_id = label_to_id  # Mapping of BIO labels to integers -> label_to_id = {'B-ING': 0, 'I-ING': 1, 'B-AMT': 2, 'O': 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Tokenize text in the same way as the ground truth (each word = 1 token)\n",
    "        text = self.data[idx]\n",
    "        labels = self.labels[idx].split()  # Assume labels are space-separated for each token -> 'B-ING I-ING O B-AMT O B-ING I-ING O B-ING O B-AMT'\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Truncate -> max length\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            labels = labels[:self.max_len]\n",
    "        else:\n",
    "            padding_len = self.max_len - len(tokens)\n",
    "            tokens += ['[PAD]'] * padding_len\n",
    "            labels += ['O'] * padding_len \n",
    "\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1 if token != '[PAD]' else 0 for token in tokens]\n",
    "        label_ids = [self.label_to_id[label] for label in labels]\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    \n",
    "# PyTorch DataLoader\n",
    "def create_dataloader(dataframe, batch_size, num_workers):\n",
    "    dataset = IngredientsDataset(dataframe)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model -> BERT and its functions(train, inference)\n",
    "Why BERT?\n",
    "- BERT: transformer-based model pre-trained on massive amounts of text data and learns contextual representations of words by considering both their left and right context (bidirectional). Really useful for this task\n",
    "- Use of 'bert-base-uncased' model: benefits from a strong understanding of language structure and semantics\n",
    "- We fine-tune the model to predict BIO entity tags for each word in the sequence (ingredients text)\n",
    "- Can handle well token-level classification, each word in the text might belong to a different category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "bio_labels = ['B-ING', 'I-ING', 'B-AMT', 'O']\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels= len(bio_labels))  # 3 classes: ingredient, quantity, other\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "    \n",
    "def train_model(dataloader, model, optimizer, loss_fn, config, device):\n",
    "    model.train()\n",
    "    for epoch in range(config['model']['epochs']):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)  # Assuming labels are provided\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{config['model']['epochs']}, Loss: {total_loss/len(dataloader)}\")\n",
    "\n",
    "# Model Inference\n",
    "def inference(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "1. Read the dataset and filter to ensure non empty ingredients_text\n",
    "2. Split training, validation, test sets -> (PyTorch) Dataloeaders\n",
    "3. Model initialisation: BERT model \n",
    "4. We add torch device to enable the use of GPUs when possible\n",
    "4. Model training: Cross-Entropy Loss is used for training with Adam optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data file\n",
    "nrows = 10000 \n",
    "df = pd.read_csv(config['data']['dataset_paths']['dataset_csv'], delimiter='\\t', on_bad_lines='skip', low_memory=False, nrows=nrows)\n",
    "\n",
    "# Preprocess data: filter out NaNs\n",
    "df = df.dropna(subset=['ingredients_text'])\n",
    "    \n",
    "# Label data addition to the dataframe using OpenAI API\n",
    "df = preprocess_and_label(df)\n",
    "\n",
    "label_to_id = {'B-ING': 0, 'I-ING': 1, 'B-AMT': 2, 'O': 3}\n",
    "# Split the dataset: 0.7 training, 0.15 val, 0.15 test\n",
    "train_df, val_df, test_df = split_data(df, config)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = create_dataloader(train_df, config['data']['batch_size'], config['data']['num_workers'])\n",
    "val_loader = create_dataloader(val_df, config['data']['batch_size'], config['data']['num_workers'])\n",
    "test_loader = create_dataloader(test_df, config['data']['batch_size'], config['data']['num_workers'])\n",
    "    \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = len(label_to_id)  \n",
    "model = TransformerModel(config).to(device)\n",
    "        \n",
    "optimizer = optim.Adam(model.parameters(), lr=config['model']['learning_rate'])\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "\n",
    "# Train the model\n",
    "train_model(train_loader, model, optimizer, loss_fn, config, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-cw1-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
